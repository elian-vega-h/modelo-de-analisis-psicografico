# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_ugXqxroYftkRnoqlhl29N-PcGKy8VL
"""

import pandas as pd
from nltk.tokenize import TweetTokenizer
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
import spacy
import en_core_web_sm
from sklearn.feature_extraction.text import TfidfVectorizer,  CountVectorizer
from sklearn.pipeline import FeatureUnion
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
import numpy as np
from scipy.stats import ttest_ind,norm
from scipy.stats import mannwhitneyu
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, recall_score, log_loss
from sklearn import linear_model,preprocessing
import random

tknzr = TweetTokenizer()

df= pd.read_csv('/content/category.csv',sep=";")


df["clean_text"] = df["content"].apply(lambda s: ' '.join(re.sub("(w+://S+)", " ", s).split()))

df["clean_text"] = df["clean_text"].apply(lambda s: ' '.join(re.sub('[.,!?:;-=''@#_]', ' ', s).split()))
df[["content","clean_text"]]

df["clean_text"].replace('d+', '', regex=True, inplace=True)
df[["content","clean_text"]]

# eliminacion de los emoji
def deEmojify(inputString):
    return inputString.encode('ascii', 'ignore').decode('ascii')
df["clean_text"] = df["clean_text"].apply(lambda s: deEmojify(s))
df[['content','clean_text']].iloc[12]

# eliminacion de las stopwords
nltk.download('stopwords')

stop = set(stopwords.words('spanish'))
print(stop)

def rem_en(input_txt):
    words = input_txt.lower().split()
    noise_free_words = [word for word in words if word not in stop] 
    noise_free_text = " ".join(noise_free_words) 
    return noise_free_text

# def tokenizar(input_txt):
#   tknzr = TweetTokenizer()
#   return tknzr.tokenize(input_txt)
  

df["clean_text"] = df["clean_text"].apply(lambda s: rem_en(s))
df[["content","clean_text"]]


# tokeniser = RegexpTokenizer(r'w+')
# df["clean_text"] = df["clean_text"].apply(lambda x: tokenizar(x))
# df[["content","clean_text"]]


# normalizacion
nlp = en_core_web_sm.load()
def normalize(text):
    doc = nlp(text)
    words = [t.orth_ for t in doc if not t.is_punct | t.is_stop]
    lexical_tokens = [t.lower() for t in words if len(t) > 3 and     
    t.isalpha()]
    return lexical_tokens

tfidf_vector = TfidfVectorizer(min_df=3, max_df=0.5, ngram_range=(1, 2))
bofngra_vector = CountVectorizer(analyzer='word', ngram_range=(1, 2))
features_union = FeatureUnion([
    ('bofngra_vector', bofngra_vector),
    ('tfidf_vector', tfidf_vector)
])

X= df["clean_text"]
y= df["categoria"]



X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.3, random_state=30)


features_union.fit(X_train)
x_train = features_union.transform(X_train)
x_test = features_union.transform(X_test)

# # Grid de hiperparámetros
# # ==============================================================================
# param_grid = {'C': np.logspace(-5, 7, 20)}

# # Búsqueda por validación cruzada
# # ==============================================================================
# grid = GridSearchCV(
#         estimator  = SVC(kernel= "rbf", gamma='scale'),
#         param_grid = param_grid,
#         scoring    = 'accuracy',
#         n_jobs     = -1,
#         cv         = 3, 
#         verbose    = 0,
#         return_train_score = True
#       )

# # Se asigna el resultado a _ para que no se imprima por pantalla
# _ = grid.fit(X = x_train, y = y_train)

# # Resultados del grid
# # ==============================================================================
# resultados = pd.DataFrame(grid.cv_results_)
# resultados.filter(regex = '(param.*|mean_t|std_t)')\
#     .drop(columns = 'params')\
#     .sort_values('mean_test_score', ascending = False) \
#     .head(5)

# Creación del modelo SVM lineal
modelo = SVC(C = 100, kernel = 'linear', random_state=123)
modelo.fit(x_train, y_train)

# Predicciones test
predicciones = modelo.predict(x_test)
predicciones

# Accuracy de test del modelo 
accuracy = accuracy_score(
            y_true    = y_test,
            y_pred    = predicciones,
            normalize = True
           )
print("")
print(f"El accuracy de test es: {100*accuracy}%")





# prueba parametrica
stat, p = ttest_ind(y_test, predicciones) 

print(f'Softmax \nEstadísticas: {stat}, pagina: {p}')
if p > 0.05:print('Misma distribución (no se rechaza H0)')
else:print('Distribución diferente (rechazar H0)')



# prueba noparametrica
estadística, pag = mannwhitneyu(y_test, predicciones)


print(f'Softmax \nEstadísticas: {estadística}, pagina: {pag}')
if pag > 0.05:print('Misma distribución (no se rechaza H0)')
else:print('Distribución diferente (rechazar H0)')
    


print(df.head())

#Softmax funtion
regs=linear_model.LogisticRegression(multi_class="multinomial",solver="lbfgs",C=1,max_iter=10000)
regs.fit(x_train,y_train)
y_prob=regs.predict(x_test)

classification = classification_report(y_test, y_prob)
print(classification)

# Modelo Random Forest
rfm=RandomForestClassifier()
rfm.fit(x_train,y_train)
y_probrf=rfm.predict(x_test)

classificationrf = classification_report(y_test, y_probrf)
print(classificationrf)